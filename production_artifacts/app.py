# -*- coding: utf-8 -*-
"""
Final Production API for ExoLife Discover
"""

import joblib
import os
import numpy as np
import pandas as pd
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from sklearn.preprocessing import LabelEncoder
from fastapi.middleware.cors import CORSMiddleware
from typing import List, Optional
import contextlib
import traceback 
import torch 
from pytorch_tabnet.tab_model import TabNetClassifier 
import io 
import torch.storage 

# =========================================================
# 0. Ø§Ù„Ø«ÙˆØ§Ø¨Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© (Constants)
# =========================================================
FINAL_FEATURE_NAMES = [
    'koi_time0bk', 'ra', 'dec', 'koi_kepmag', 'default_flag', 'disposition', 
    'sy_snum', 'sy_pnum', 'discoverymethod', 'disc_year', 'disc_facility', 
    'soltype', 'pl_orbsmax', 'pl_orbeccen', 'ttv_flag', 'st_spectype', 
    'st_mass', 'st_met', 'st_metratio', 'sy_vmag', 'sy_kmag', 'sy_gaiamag', 
    'st_pmra', 'st_pmdec', 'pl_trandurh', 'pl_trandep', 'st_tmag', 
    'Orbital_Period', 'Planet_Radius_R_E', 'Planet_Mass_M_E', 
    'Equilibrium_Temp_K', 'Stellar_Teff_K', 'Planet_Density_Ratio'
]
NUM_FEATURES = len(FINAL_FEATURE_NAMES)
EXPORT_DIR = 'production_artifacts'
model_artifacts = {}
CLASS_LABELS = {0: 'CANDIDATE', 1: 'CONFIRMED', 2: 'FALSE POSITIVE'}

# =========================================================
# 1. ØªØ¹Ø±ÙŠÙ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ (Pydantic Schema)
# =========================================================
class PredictionInput(BaseModel):
    # Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù€ 33 Ù‡Ù†Ø§ 
    koi_time0bk: Optional[float] = None
    ra: Optional[float] = None
    dec: Optional[float] = None
    koi_kepmag: Optional[float] = None
    default_flag: Optional[float] = None
    disposition: Optional[str] = None
    sy_snum: Optional[float] = None
    sy_pnum: Optional[float] = None
    discoverymethod: Optional[str] = None
    disc_year: Optional[float] = None
    disc_facility: Optional[str] = None
    soltype: Optional[str] = None
    pl_orbsmax: Optional[float] = None
    pl_orbeccen: Optional[float] = None
    ttv_flag: Optional[float] = None
    st_spectype: Optional[str] = None
    st_mass: Optional[float] = None
    st_met: Optional[float] = None
    st_metratio: Optional[float] = None
    sy_vmag: Optional[float] = None
    sy_kmag: Optional[float] = None
    sy_gaiamag: Optional[float] = None
    st_pmra: Optional[float] = None
    st_pmdec: Optional[float] = None
    pl_trandurh: Optional[float] = None
    pl_trandep: Optional[float] = None
    st_tmag: Optional[float] = None
    Orbital_Period: Optional[float] = None
    Planet_Radius_R_E: Optional[float] = None
    Planet_Mass_M_E: Optional[float] = None
    Equilibrium_Temp_K: Optional[float] = None
    Stellar_Teff_K: Optional[float] = None
    Planet_Density_Ratio: Optional[float] = None


# =========================================================
# 2. ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„Ù€ Pipeline (Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙØ³Ø¨Ù‚Ø©) - Ø§Ù„Ø£Ù‡Ù…!
# =========================================================
def production_pipeline(raw_input_data, scaler_obj, pca_obj):
    # Ø¥Ù†Ø´Ø§Ø¡ DataFrame Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø®Ø§Ù…
    df_processed = pd.DataFrame([raw_input_data])
    
    # ğŸš¨ Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª: Ø­Ø³Ø§Ø¨ Planet_Density_Ratio 
    with contextlib.suppress(Exception): 
        mass = df_processed['Planet_Mass_M_E'].iloc[0]
        radius = df_processed['Planet_Radius_R_E'].iloc[0]
        
        # Ù†Ø­Ø³Ø¨ Ø§Ù„ÙƒØ«Ø§ÙØ© ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚ÙŠÙ… Ù…Ù†Ø·Ù‚ÙŠØ© ÙˆÙ…ÙˆØ¬ÙˆØ¯Ø© ÙˆØºÙŠØ± ØµÙØ±
        if mass is not None and radius is not None and radius != 0:
            df_processed['Planet_Density_Ratio'] = mass / (radius ** 3)
        
    # 1. Ø§Ù„ØªØµÙÙŠØ©: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù€ 33 Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø§Ù„Ø¶Ø¨Ø· (Ù…Ù‡Ù… Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨)
    X_predict = df_processed.reindex(columns=FINAL_FEATURE_NAMES)
    
    # 2. Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© (Imputation): Ù†Ù…Ù„Ø£ NaN Ø¨Ù€ 0
    X_predict = X_predict.fillna(0) 

    # 3. ØªØ±Ù…ÙŠØ² ÙØ¦ÙˆÙŠ (Label Encoding) Ù„Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©
    categorical_cols = X_predict.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        le = LabelEncoder()
        # Ù†Ù…Ù„Ø£ Ø£ÙŠ Ù‚ÙŠÙ…Ø© Ù…ÙÙ‚ÙˆØ¯Ø© 'NaN' ÙÙŠ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ© Ø¨ÙƒÙ„Ù…Ø© 'missing' Ù‚Ø¨Ù„ Ø§Ù„ØªØ±Ù…ÙŠØ²
        X_predict[col] = le.fit_transform(X_predict[col].astype(str).fillna('missing'))

    # 4. Ø§Ù„ØªØ­Ø¬ÙŠÙ… (Scaling)
    X_scaled_np = scaler_obj.transform(X_predict.values)
    
    # 5. ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ (PCA)
    X_pca_np = pca_obj.transform(X_scaled_np).astype(np.float32)
    
    return X_pca_np


# =========================================================
# 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ FastAPI Ùˆ Lifespan (Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª)
# =========================================================

# ØªØ®Ø²ÙŠÙ† Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ù€ PyTorch Ù‚Ø¨Ù„ Ø§Ù„ØªØ±Ù‚ÙŠØ¹
original_load_from_bytes = torch.storage._load_from_bytes

# ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ±Ù‚ÙŠØ¹: ØªØ¬Ø¨Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¹Ù„Ù‰ CPU
def _patched_load_from_bytes(b):
    # Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙØ© ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© joblib/pickle Ø¹Ù†Ø¯ Ù…Ø­Ø§ÙˆÙ„Ø© PyTorch
    # Ù„Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø¨Ø§ÙŠØªØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ù‡Ù†Ø§ Ù†Ø¶Ù…Ù† ØªÙ…Ø±ÙŠØ± map_location='cpu'
    return torch.load(io.BytesIO(b), map_location=torch.device('cpu'), weights_only=False)

# ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© lifespan (ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„)
@contextlib.asynccontextmanager
async def lifespan(app: FastAPI):
    global model_artifacts
    
    # Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ±Ù‚ÙŠØ¹ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø¹Ù„Ù‰ CPU
    # Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø¶Ø±ÙˆØ±ÙŠ Ù„Ù€ TabNet Ø§Ù„Ù…Ø­ÙÙˆØ¸ Ø¹Ù„Ù‰ GPU ÙˆÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ø¹Ù„Ù‰ CPU
    torch.storage._load_from_bytes = _patched_load_from_bytes
    
    try:
        # ğŸš¨ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ TabNet Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… joblib.load
        # Ø§Ù„ØªØ±Ù‚ÙŠØ¹ Ø£Ø¹Ù„Ø§Ù‡ ÙŠØ¶Ù…Ù† Ø£Ù† Ø£ÙŠ PyTorch load ÙŠØªÙ… Ø¯Ø§Ø®Ù„ joblib Ø³ÙŠØ³ØªØ®Ø¯Ù… CPU
        print("Attempting to load TabNet model...")
        model_artifacts['model'] = joblib.load(os.path.join(EXPORT_DIR, 'tabnet_model_tn2.joblib'))
        
        # ØªØ­Ù…ÙŠÙ„ Scaler Ùˆ PCA Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… joblib.load
        print("Loading Scaler and PCA...")
        model_artifacts['scaler'] = joblib.load(os.path.join(EXPORT_DIR, 'scaler_for_api.joblib'))
        model_artifacts['pca'] = joblib.load(os.path.join(EXPORT_DIR, 'pca_for_api.joblib'))
        
        # âŒ ØªÙ…Øª Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø³Ø·Ø± model_artifacts['model'].model.eval()
        # Ù„Ø£Ù† Ø§Ù„ÙƒØ§Ø¦Ù† TabNetClassifier ÙŠÙØ³ØªØ®Ø¯Ù… Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙ„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ .model Ø¯Ø§Ø¦Ù…Ù‹Ø§
        
        print("ğŸš€ All ML artifacts loaded successfully. Server is ready for predictions.")
    except Exception as e:
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø£ØµÙ„ÙŠ ÙƒØ§Ù…Ù„Ø§Ù‹
        traceback.print_exc() 
        print(f"Error loading ML artifacts: {e}")
        # Ø±ÙØ¹ Ø§Ø³ØªØ«Ù†Ø§Ø¡ 500 ÙÙŠ Ø­Ø§Ù„Ø© ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ´ØºÙŠÙ„
        raise HTTPException(status_code=500, detail="ML model components failed to load. Check server logs.")
    finally:
        # Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù…ÙŠÙ„
        torch.storage._load_from_bytes = original_load_from_bytes
        
    yield

# Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ FastAPI
app = FastAPI(lifespan=lifespan, title="ExoLife Discover API", version="1.0.0")

# ğŸ›¡ï¸ Ø¥Ø¶Ø§ÙØ© Ø­Ù…Ø§ÙŠØ© CORS (Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ù† ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ÙˆÙŠØ¨)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)


# =========================================================
# 4. Ù†Ù‚Ø·Ø© Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Endpoint) /predict
# =========================================================

@app.post("/predict", summary="Predicts Exoplanet Status")
def predict(input_data: PredictionInput):
    # 1. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø­ÙÙ…Ù„Øª
    if not model_artifacts.get('model'):
        raise HTTPException(status_code=503, detail="Model server not ready or components failed to load.")
        
    # ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Pydantic Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³ Ø®Ø§Ù…
    raw_dict = input_data.dict()
    
    try:
        # 2. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù€ Pipeline
        processed_features = production_pipeline(
            raw_dict,
            model_artifacts['scaler'],
            model_artifacts['pca']
        )
        
        # 3. Ø§Ù„ØªÙˆÙ‚Ø¹ (Ø§Ø³ØªØ®Ø¯Ø§Ù… TabNet's native predict/predict_proba)
        prediction = model_artifacts['model'].predict(processed_features)
        probabilities = model_artifacts['model'].predict_proba(processed_features)
        
        # 4. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø¯
        predicted_class_index = int(prediction[0])
        
        response = {
            "status": "success",
            "predicted_class": CLASS_LABELS.get(predicted_class_index, "UNKNOWN"),
            "confidence_scores": {
                "CANDIDATE": f"{probabilities[0][0]*100:.2f}%",
                "CONFIRMED": f"{probabilities[0][1]*100:.2f}%",
                "FALSE_POSITIVE": f"{probabilities[0][2]*100:.2f}%"
            },
            "raw_prediction_index": predicted_class_index
        }
        
        return response
    
    except Exception as e:
        print(f"Prediction processing failed: {e}")
        raise HTTPException(status_code=500, detail=f"Prediction processing error: {e}")

# =========================================================
# 5. Ù†Ù‚Ø·Ø© Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Endpoint) /
# =========================================================
@app.get("/")
def read_root():
    return {"message": "ExoLife Discover API is running! Access /docs for endpoints."}
